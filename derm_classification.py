# -*- coding: utf-8 -*-
"""Derm_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yFNx6pjZlu37MFNY19AIV9SI0MGeqyVF

**Imports**
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import keras
import tensorflow as tf
import random
from tensorflow.image import rgb_to_grayscale
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier
from keras.preprocessing.image import ImageDataGenerator
from sklearn import svm
from sklearn.naive_bayes import GaussianNB

"""**Load Data**"""

!wget -O Data.npz https://zenodo.org/record/4269852/files/dermamnist.npz?download=1

data = np.load('Data.npz')
print(data.files, "\n")

print(data['train_images'].shape)
print(data['train_images'].ndim, "\n")

print(data['val_images'].shape)
print(data['val_images'].ndim, "\n")

print(data['test_images'].shape)
print(data['test_images'].ndim, "\n")

#Podzia≈Ç 70% train, 10% val, 20% test

print(data['train_labels'].shape)
print(data['train_labels'].ndim, "\n")

print(data['val_labels'].shape)
print(data['val_labels'].ndim, "\n")

print(data['test_labels'].shape)
print(data['test_labels'].ndim, "\n")

images_train = data['train_images']
labels_train = data['train_labels']

num_examples_train = 7
fig, axes = plt.subplots(1, num_examples_train, figsize=(15, 2.5))
fig.suptitle("Train examples")

for i in range(num_examples_train):
    axes[i].imshow(images_train[i])
    axes[i].set_title('Label: {}'.format(labels_train[i][0]))
    axes[i].axis('off')
plt.show()



images_val = data['val_images']
labels_val = data['val_labels']

num_examples_val = 7
fig, axes = plt.subplots(1, num_examples_val, figsize=(15, 2.5))
fig.suptitle("Val examples")

for i in range(num_examples_val):
    axes[i].imshow(images_val[i])
    axes[i].set_title('Label: {}'.format(labels_val[i][0]))
    axes[i].axis('off')
plt.show()



images_test = data['test_images']
labels_test = data['test_labels']

num_examples_test = 7
fig, axes = plt.subplots(1, num_examples_test, figsize=(15, 2.5))
fig.suptitle("Test examples")

for i in range(num_examples_test):
    axes[i].imshow(images_test[i])
    axes[i].set_title('Label: {}'.format(labels_test[i][0]))
    axes[i].axis('off')
plt.show()

labels_train = labels_train.reshape(data['train_images'].shape[0])#reshape klas treningowych z dwywymiarowej macierzy do jednowymiarowego wektora
labels_val = labels_val.reshape(data['val_images'].shape[0])#reshape klas walidacyjnych z dwywymiarowej macierzy do jednowymiarowego wektora
labels_test = labels_test.reshape(data['test_images'].shape[0])#reshape klas testowych z dwywymiarowej macierzy do jednowymiarowego wektora

print(labels_train.shape, labels_train.ndim)
print(labels_val.shape, labels_val.ndim)
print(labels_test.shape, labels_test.ndim)

"""**Number of records for individual classes**"""

train_class_counts = np.bincount(labels_train)
print(train_class_counts)

class_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']

plt.figure(figsize=(4, 3))
sns.barplot(x=class_labels, y=train_class_counts)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Count Train')
plt.show()



val_class_counts = np.bincount(labels_val)
print(val_class_counts)

class_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']

plt.figure(figsize=(4, 3))
sns.barplot(x=class_labels, y=val_class_counts)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Count Val')
plt.show()



test_class_counts = np.bincount(labels_test)
print(test_class_counts)

class_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']

plt.figure(figsize=(4, 3))
sns.barplot(x=class_labels, y=test_class_counts)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Count Test')
plt.show()

"""**Preprocessing for training set (grayscale, flattening, normalization)**"""

train_features = tf.image.rgb_to_grayscale(images_train)
train_labels_gray = labels_train
train_images_gray = train_features
random_state = 42
print(train_images_gray.shape)

num_examples_gray = 7
fig, axes = plt.subplots(1, num_examples_gray, figsize=(10, 2))
for i in range(num_examples_gray):
    axes[i].imshow(train_images_gray[i], cmap="gray")
    axes[i].set_title('Label: {}'.format(train_labels_gray[i]))
    axes[i].axis('off')

plt.show()
gray_train_features = train_features.numpy()
print(gray_train_features.shape, "\n")

flat_train_features = np.zeros((gray_train_features.shape[0], gray_train_features.shape[1] * gray_train_features.shape[2]))
print(flat_train_features.shape, "\n")

for i in range(gray_train_features.shape[0]):
    flat_train_features[i] = gray_train_features[i].reshape(gray_train_features.shape[1] * gray_train_features.shape[2])

print(flat_train_features.shape, "\n")
print(flat_train_features)

flat_train_scaled = flat_train_features

flat_train_scaled = np.round(flat_train_features / 255.0, 3)
flat_train_scaled = tf.image.convert_image_dtype(flat_train_scaled, tf.float32)

print(flat_train_scaled.shape, "\n")
print(flat_train_scaled)

"""**Preprocessing for test set (grayscale, flattening, normalization)**"""

test_features = tf.image.rgb_to_grayscale(images_test)
test_labels_gray = labels_test
test_images_gray = test_features
print(test_images_gray.shape)

num_examples_gray = 7
fig, axes = plt.subplots(1, num_examples_gray, figsize=(10, 2))
for i in range(num_examples_gray):
    axes[i].imshow(test_images_gray[i], cmap="gray")
    axes[i].set_title('Label: {}'.format(test_labels_gray[i]))
    axes[i].axis('off')

plt.show()
gray_test_features = test_features.numpy()
print(gray_test_features.shape,"\n")

flat_test_features = np.zeros((gray_test_features.shape[0], gray_test_features.shape[1] * gray_test_features.shape[2]))
print(flat_test_features.shape, "\n")

for i in range(gray_test_features.shape[0]):
    flat_test_features[i] = gray_test_features[i].reshape(gray_test_features.shape[1] * gray_test_features.shape[2])

print(flat_test_features.shape, "\n")
print(flat_test_features)

flat_test_scaled = flat_test_features

flat_test_scaled = np.round(flat_test_features / 255.0, 3)
flat_test_scaled = tf.image.convert_image_dtype(flat_test_scaled, tf.float32)

df_test_features = pd.DataFrame(flat_test_scaled)
df_test_labels = pd.DataFrame(test_labels_gray)
df_test_labels = df_test_labels.rename(columns={0: 'labels'})
df_test = pd.concat([df_test_features, df_test_labels], axis=1)

df_test_split = df_test.copy()

X_test = df_test_split.drop(['labels'], axis=1)
y_test = df_test_split['labels']

print(X_test.shape)
print(y_test.shape)
df_test_split

"""**Final preparation of data for the classifier**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt

df_train_features = pd.DataFrame(flat_train_scaled)
df_train_labels = pd.DataFrame(train_labels_gray)
df_train_labels = df_train_labels.rename(columns={0: 'labels'})
df_train = pd.concat([df_train_features, df_train_labels], axis=1)

X_train = df_train.drop(['labels'], axis=1)
y_train = df_train['labels']

X_test = df_test_split.drop(['labels'], axis=1)
y_test = df_test_split['labels']

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

optimizer = keras.optimizers.Adam(learning_rate=0.0005)

model=Sequential()

model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Dropout(0.2))

model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Dense(7, activation='softmax'))
model.compile(optimizer = 'Adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

model.summary()

learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_accuracy',
    patience=3,
    verbose=1,
    factor=0.5,
    min_lr=0.00001
)
callbacks = [learning_rate_reduction]

history = model.fit(
    X_train, y_train,
    epochs=80,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=callbacks
)

model.save('trained_model.h5')

loaded_model = load_model('trained_model.h5')

y_pred = loaded_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

accuracy = metrics.accuracy_score(y_true, y_pred_classes)
precision = metrics.precision_score(y_true, y_pred_classes, average='weighted')
recall = metrics.recall_score(y_true, y_pred_classes, average='weighted')
f1 = metrics.f1_score(y_true, y_pred_classes, average='weighted')

print("Accuracy:", round(accuracy, 3))
print("Precision:", round(precision, 3))
print("Recall:", round(recall, 3))
print("F1 Score:", round(f1, 3))

cm = metrics.confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

classification_report_result = metrics.classification_report(y_true, y_pred_classes)
print("Classification Report:")
print(classification_report_result)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()